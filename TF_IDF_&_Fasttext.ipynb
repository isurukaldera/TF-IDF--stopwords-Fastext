{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DJUN2wu8O4df",
        "outputId": "3978a959-d0a5-42f2-843f-56f27598a9a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install pandas"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-learn\n"
      ],
      "metadata": {
        "id": "KoJngPxBPHMB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a69858dc-7d1e-4010-9bac-f2bffe5bec86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.5.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fasttext"
      ],
      "metadata": {
        "id": "N8RytyiNPiSu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef788932-14ba-4d2c-c4da-1258d06e7c39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fasttext\n",
            "  Downloading fasttext-0.9.3.tar.gz (73 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/73.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.4/73.4 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pybind11>=2.2 (from fasttext)\n",
            "  Using cached pybind11-2.13.6-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from fasttext) (75.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from fasttext) (1.26.4)\n",
            "Using cached pybind11-2.13.6-py3-none-any.whl (243 kB)\n",
            "Building wheels for collected packages: fasttext\n",
            "  Building wheel for fasttext (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.3-cp311-cp311-linux_x86_64.whl size=4313472 sha256=13e8460184e8ba6b4732fb2d0901d336ba973fc59f8d4457cfbab9097545dbbd\n",
            "  Stored in directory: /root/.cache/pip/wheels/65/4f/35/5057db0249224e9ab55a513fa6b79451473ceb7713017823c3\n",
            "Successfully built fasttext\n",
            "Installing collected packages: pybind11, fasttext\n",
            "Successfully installed fasttext-0.9.3 pybind11-2.13.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install NLTK"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p35-EeMcv3Jm",
        "outputId": "b0e26fda-5000-402a-c1e4-e4f8afa2c894"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: NLTK in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from NLTK) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from NLTK) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from NLTK) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from NLTK) (4.67.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X_W9NicmGiLP",
        "outputId": "c70692a0-03cf-499e-aba5-b4456342960b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "import fasttext\n",
        "import string\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "train_data = pd.read_csv(\"Train.csv\")\n",
        "test_data = pd.read_csv(\"Test.csv\")\n",
        "test_data = pd.read_csv(\"Valid.csv\")\n",
        "\n",
        "print(train_data.info())\n",
        "print(test_data.info())\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "def preprocess_text(text):\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    tokens = [word for word in tokens if word not in stop_words and word not in string.punctuation]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "train_data['preprocessed_text'] = train_data['text'].apply(preprocess_text)\n",
        "test_data['preprocessed_text'] = test_data['text'].apply(preprocess_text)\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=20000)\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(train_data['text'])\n",
        "X_test_tfidf = tfidf_vectorizer.transform(test_data['text'])\n",
        "y_train = train_data['label']\n",
        "y_test = test_data['label']\n",
        "\n",
        "tfidf_stop_vectorizer = TfidfVectorizer(max_features=5000, stop_words=\"english\")\n",
        "X_train_tfidf_stop = tfidf_stop_vectorizer.fit_transform(train_data['preprocessed_text'])\n",
        "X_test_tfidf_stop = tfidf_stop_vectorizer.transform(test_data['preprocessed_text'])\n",
        "\n",
        "lr_model = LogisticRegression()\n",
        "lr_model.fit(X_train_tfidf, y_train)\n",
        "y_pred_lr = lr_model.predict(X_test_tfidf)\n",
        "\n",
        "print(\"Classification report Approach 1 (TF-IDF without stop words):\")\n",
        "print(classification_report(y_test, y_pred_lr, target_names=['Class 0', 'Class 1'], digits=4))\n",
        "\n",
        "lr_stop_model = LogisticRegression()\n",
        "lr_stop_model.fit(X_train_tfidf_stop, y_train)\n",
        "y_pred_lr_stop = lr_stop_model.predict(X_test_tfidf_stop)\n",
        "\n",
        "print(\"Classification report Approach 2 (TF-IDF with stop words removed):\")\n",
        "print(classification_report(y_test, y_pred_lr_stop, target_names=['Class 0', 'Class 1'], digits=4))\n",
        "\n",
        "with open('train.txt', 'w', encoding='utf-8') as f:\n",
        "    for label, text in zip(train_data['label'], train_data['text']):\n",
        "        f.write(f\"__label__{label} {text}\\n\")\n",
        "\n",
        "with open('test.txt', 'w', encoding='utf-8') as f:\n",
        "    for label, text in zip(test_data['label'], test_data['text']):\n",
        "        f.write(f\"__label__{label} {text}\\n\")\n",
        "\n",
        "ft_model = fasttext.train_supervised(input='train.txt', epoch=25, lr=1.0, wordNgrams=2)\n",
        "test_texts = test_data['text'].tolist()\n",
        "predictions = [ft_model.predict(text)[0][0] for text in test_texts]\n",
        "\n",
        "\n",
        "y_pred_ft = [1 if pred == '__label__1' else 0 for pred in predictions]\n",
        "y_true_ft = ['__label__' + str(label) for label in y_test]\n",
        "\n",
        "print(\"Classification report Approach 3 (FastText):\")\n",
        "print(classification_report(y_true_ft, predictions, target_names=['Class 0', 'Class 1'], digits=4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S8z94vOaGusM",
        "outputId": "8464166c-cd1e-4798-ab63-fae169b3798e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 40000 entries, 0 to 39999\n",
            "Data columns (total 2 columns):\n",
            " #   Column  Non-Null Count  Dtype \n",
            "---  ------  --------------  ----- \n",
            " 0   text    40000 non-null  object\n",
            " 1   label   40000 non-null  int64 \n",
            "dtypes: int64(1), object(1)\n",
            "memory usage: 625.1+ KB\n",
            "None\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 5000 entries, 0 to 4999\n",
            "Data columns (total 2 columns):\n",
            " #   Column  Non-Null Count  Dtype \n",
            "---  ------  --------------  ----- \n",
            " 0   text    5000 non-null   object\n",
            " 1   label   5000 non-null   int64 \n",
            "dtypes: int64(1), object(1)\n",
            "memory usage: 78.3+ KB\n",
            "None\n",
            "Classification report Approach 1 (TF-IDF without stop words):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     Class 0     0.8999    0.8749    0.8872      2486\n",
            "     Class 1     0.8796    0.9037    0.8915      2514\n",
            "\n",
            "    accuracy                         0.8894      5000\n",
            "   macro avg     0.8897    0.8893    0.8894      5000\n",
            "weighted avg     0.8897    0.8894    0.8894      5000\n",
            "\n",
            "Classification report Approach 2 (TF-IDF with stop words removed):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     Class 0     0.8911    0.8656    0.8782      2486\n",
            "     Class 1     0.8708    0.8954    0.8829      2514\n",
            "\n",
            "    accuracy                         0.8806      5000\n",
            "   macro avg     0.8809    0.8805    0.8806      5000\n",
            "weighted avg     0.8809    0.8806    0.8806      5000\n",
            "\n",
            "Classification report Approach 3 (FastText):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     Class 0     0.8991    0.8858    0.8924      2486\n",
            "     Class 1     0.8887    0.9018    0.8952      2514\n",
            "\n",
            "    accuracy                         0.8938      5000\n",
            "   macro avg     0.8939    0.8938    0.8938      5000\n",
            "weighted avg     0.8939    0.8938    0.8938      5000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "f37sjhmCHRGO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}